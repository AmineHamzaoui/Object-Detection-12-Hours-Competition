{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"rLeEsdNNx02O"},"outputs":[],"source":["!pip install -q kaggle"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"UO4jlCUcyst2","outputId":"93dd4224-a996-43e2-ff01-84a29dbe72ae"},"outputs":[{"data":{"text/html":["\n","     <input type=\"file\" id=\"files-bea0eae2-0435-446c-8d29-5cb1c4ecbe6e\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-bea0eae2-0435-446c-8d29-5cb1c4ecbe6e\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["from google.colab import files\n","files.upload()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ro6l3pbI-Ows"},"outputs":[],"source":["!mkdir ~/.kaggle\n","!cp kaggle.json ~/.kaggle/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EDdDnars-S0m"},"outputs":[],"source":["!chmod 600 ~/.kaggle/kaggle.json"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":811,"status":"ok","timestamp":1708216407723,"user":{"displayName":"Hamzaoui Mohamed Amine","userId":"09072350114090446150"},"user_tz":-60},"id":"qOqEli2o-VIv","outputId":"5bc58674-df26-4e79-df9a-8a654989ca67"},"outputs":[{"name":"stdout","output_type":"stream","text":["Warning: Looks like you're using an outdated API Version, please consider updating (server 1.6.6 / client 1.5.16)\n","ref                                                        title                                              size  lastUpdated          downloadCount  voteCount  usabilityRating  \n","---------------------------------------------------------  ------------------------------------------------  -----  -------------------  -------------  ---------  ---------------  \n","nelgiriyewithana/apple-quality                             Apple Quality                                     170KB  2024-01-11 14:31:07          15959        360  1.0              \n","bhavikjikadara/google-play-store-applications              Google Play Store Applications                    328KB  2024-02-13 06:48:02           1176         35  1.0              \n","nelgiriyewithana/emotions                                  Emotions                                           16MB  2024-02-05 16:01:39           2087         81  1.0              \n","joebeachcapital/valentines-day-consumer-data               Valentine's Day Consumer Data                       1KB  2024-02-12 00:21:31           1541         35  1.0              \n","joebeachcapital/chocolate-ratings                          Chocolate Ratings                                  64KB  2024-02-12 00:52:44           1412         34  1.0              \n","mrsimple07/car-prices-prediction-data                      Car-Prices-Prediction-data                         14KB  2024-02-10 10:40:23           1634         40  1.0              \n","imyjoshua/average-time-spent-by-a-user-on-social-media     Average Time Spent By A User On Social Media       11KB  2024-01-18 10:18:03           3195         45  1.0              \n","agarwalyashhh/best-selling-books                           Best Selling Books                                  9KB  2024-02-08 14:37:00           1214         27  1.0              \n","elvinrustam/coursera-dataset                               Coursera Dataset                                    4MB  2024-02-12 19:41:40            850         31  1.0              \n","beridzeg45/video-games                                     🎮Video Games Dataset                              756KB  2024-02-10 05:16:28            666         22  1.0              \n","rrkcoder/swiggy-restaurants-dataset                        Swiggy Restaurant's Dataset                         3MB  2024-02-10 16:49:50           1045         51  1.0              \n","bhavikjikadara/loan-status-prediction                      Loan Status Prediction                              5KB  2024-02-02 13:27:20           2246         50  1.0              \n","shiivvvaam/top-youtuber-worldwide                          Top YouTubers Worldwide                            83KB  2024-02-05 09:33:23           1521         59  1.0              \n","dansbecker/melbourne-housing-snapshot                      Melbourne Housing Snapshot                        451KB  2018-06-05 12:52:24         138815       1432  0.7058824        \n","shiivvvaam/most-watched-movies-and-tv-shows                Most Watched Movies and TV Shows                    1MB  2024-02-04 19:19:18           2199         60  1.0              \n","amitvkulkarni/hair-health                                  Hair Health Prediction                             14KB  2024-02-07 13:21:14            651         28  1.0              \n","yukawithdata/spotify-top-tracks-2023                       Spotify Top 50 Tracks 2023                          4KB  2024-02-07 05:52:12           1337         29  1.0              \n","kanchana1990/2024-amazon-best-sellers-top-valentine-gifts  2024 Amazon Best Sellers: Top Valentine Gifts 🎯💘   25KB  2024-02-06 18:49:26           1338         38  1.0              \n","nimapourmoradi/heart-failure-clinical-records              heart failure clinical records                      4KB  2024-02-10 14:29:36            222         62  1.0              \n","hummaamqaasim/jobs-in-data                                 Jobs and Salaries in Data Science                  76KB  2023-12-25 20:03:32          18737        298  1.0              \n"]}],"source":["!kaggle datasets list"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2578,"status":"ok","timestamp":1708216411154,"user":{"displayName":"Hamzaoui Mohamed Amine","userId":"09072350114090446150"},"user_tz":-60},"id":"AErPRcKB-WVZ","outputId":"930c83d2-6836-42ce-8bfa-413c3140a486"},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading gods30.zip to /content\n"," 87% 128M/147M [00:01<00:00, 117MB/s] \n","100% 147M/147M [00:01<00:00, 114MB/s]\n"]}],"source":["!kaggle competitions download -c gods30"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZNWA68YBWfHM"},"outputs":[],"source":["repository_path = \"/content/extracted_folder\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HrfjYIlxWou7"},"outputs":[],"source":["import zipfile\n","\n","with zipfile.ZipFile(\"gods30.zip\", \"r\") as zip_ref:\n","    zip_ref.extractall(\"extracted_folder\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":244,"status":"ok","timestamp":1708218049547,"user":{"displayName":"Hamzaoui Mohamed Amine","userId":"09072350114090446150"},"user_tz":-60},"id":"mS5M5KXY_TGV","outputId":"ea289557-0e1b-4fb3-e370-82efe0b04f77"},"outputs":[{"data":{"text/plain":["196"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["import numpy as np\n","import pandas as pd\n","test_csv = pd.read_csv(\"/content/extracted_folder/Test.csv\")\n","test_csv.head()\n","len(test_csv)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":372,"status":"ok","timestamp":1708218050896,"user":{"displayName":"Hamzaoui Mohamed Amine","userId":"09072350114090446150"},"user_tz":-60},"id":"BAPcXFZv_66F","outputId":"94a1aac8-20ea-47dd-e9c1-ee796c6c6db8"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"summary":"{\n  \"name\": \"train_csv\",\n  \"rows\": 2603,\n  \"fields\": [\n    {\n      \"column\": \"filename\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"samples\": [\n          \"youtube-384_jpg.rf.4fa86dcb19a1b3ece13dd744e3e1fa77.jpg\",\n          \"image_50_jpg.rf.8319a3a21c0ca02bac0a8722b6f6ce47.jpg\",\n          \"2008_008652_jpg.rf.b521f230883c06887779ea1261f38ea0.jpg\"\n        ],\n        \"num_unique_values\": 2603,\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \" Hardhat\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"num_unique_values\": 2,\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \" Mask\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"num_unique_values\": 2,\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \" NO-Hardhat\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"num_unique_values\": 2,\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \" NO-Mask\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"num_unique_values\": 2,\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \" NO-Safety Vest\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"num_unique_values\": 2,\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \" Person\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"num_unique_values\": 2,\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \" Safety Cone\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"num_unique_values\": 2,\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \" Safety Vest\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"num_unique_values\": 2,\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \" machinery\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"num_unique_values\": 2,\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \" vehicle\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"num_unique_values\": 2,\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}","type":"dataframe","variable_name":"train_csv"},"text/html":["\n","  <div id=\"df-b7548f89-ccc3-4cf2-92e1-eb509e949cba\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>filename</th>\n","      <th>Hardhat</th>\n","      <th>Mask</th>\n","      <th>NO-Hardhat</th>\n","      <th>NO-Mask</th>\n","      <th>NO-Safety Vest</th>\n","      <th>Person</th>\n","      <th>Safety Cone</th>\n","      <th>Safety Vest</th>\n","      <th>machinery</th>\n","      <th>vehicle</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>youtube-348_jpg.rf.c478843562d5236a46c6448ecc3...</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>youtube-196_jpg.rf.c4db8b391b275b84d6183839008...</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>construction-1-_mp4-46_jpg.rf.c4db8e0500825ba1...</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>youtube-710_jpg.rf.c4e2b15fb3e7fac762b9a0f5457...</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>construction-2-_mp4-15_jpg.rf.c4fcd7ed43e0f6c2...</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b7548f89-ccc3-4cf2-92e1-eb509e949cba')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-b7548f89-ccc3-4cf2-92e1-eb509e949cba button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-b7548f89-ccc3-4cf2-92e1-eb509e949cba');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-07104697-7d18-44fc-9225-440974e6fba4\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-07104697-7d18-44fc-9225-440974e6fba4')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-07104697-7d18-44fc-9225-440974e6fba4 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"],"text/plain":["                                            filename   Hardhat   Mask  \\\n","0  youtube-348_jpg.rf.c478843562d5236a46c6448ecc3...         1      1   \n","1  youtube-196_jpg.rf.c4db8b391b275b84d6183839008...         1      0   \n","2  construction-1-_mp4-46_jpg.rf.c4db8e0500825ba1...         1      1   \n","3  youtube-710_jpg.rf.c4e2b15fb3e7fac762b9a0f5457...         1      1   \n","4  construction-2-_mp4-15_jpg.rf.c4fcd7ed43e0f6c2...         0      1   \n","\n","    NO-Hardhat   NO-Mask   NO-Safety Vest   Person   Safety Cone  \\\n","0            0         1                1        1             0   \n","1            1         1                1        1             1   \n","2            1         0                1        1             0   \n","3            0         1                1        1             0   \n","4            0         0                1        1             0   \n","\n","    Safety Vest   machinery   vehicle  \n","0             1           1         1  \n","1             1           1         0  \n","2             0           1         0  \n","3             1           1         0  \n","4             0           1         0  "]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["train_csv = pd.read_csv(\"/content/extracted_folder/Train.csv\")\n","train_csv.head()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":262,"status":"ok","timestamp":1708216418519,"user":{"displayName":"Hamzaoui Mohamed Amine","userId":"09072350114090446150"},"user_tz":-60},"id":"dKFBgMMzKceh","outputId":"60ebbfc1-d114-4f0f-e2d2-99346d06aa9b"},"outputs":[{"name":"stdout","output_type":"stream","text":["The following columns are missing: ['Hardhat', 'Mask', 'NO-Hardhat', 'NO-Mask', 'NO-Safety Vest', 'Person', 'Safety Cone', 'Safety Vest', 'machinery', 'vehicle']\n"]}],"source":["# Define the expected column names\n","cols = [\"filename\", \"Hardhat\", \"Mask\", \"NO-Hardhat\", \"NO-Mask\", \"NO-Safety Vest\", \"Person\", \"Safety Cone\", \"Safety Vest\", \"machinery\", \"vehicle\"]\n","\n","# Check if all expected columns are present in the DataFrame\n","missing_cols = [col for col in cols if col not in train_csv.columns]\n","\n","if missing_cols:\n","    print(\"The following columns are missing:\", missing_cols)\n","else:\n","    print(\"All columns are present.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q_5GLWLL_7Cj"},"outputs":[],"source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Activation,Conv2D, Flatten, Dropout, MaxPooling2D, BatchNormalization\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from keras import regularizers, optimizers\n","import os\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":326,"status":"ok","timestamp":1708218055832,"user":{"displayName":"Hamzaoui Mohamed Amine","userId":"09072350114090446150"},"user_tz":-60},"id":"fRTvncrXJgRh","outputId":"f7642d8e-d52b-4463-b311-fbe3d4ace50e"},"outputs":[{"data":{"text/plain":["False"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["cols = [\"Hardhat\", \"Mask\", \"NO-Hardhat\", \"NO-Mask\", \"NO-Safety Vest\", \"Person\", \"Safety Cone\", \"Safety Vest\", \"machinery\", \"vehicle\"]\n","set(train_csv.columns).issuperset(cols)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":359,"status":"ok","timestamp":1708216422275,"user":{"displayName":"Hamzaoui Mohamed Amine","userId":"09072350114090446150"},"user_tz":-60},"id":"fSGDTbzbKH3s","outputId":"1b1a271f-a465-47c4-8d0d-08290fa5beb1"},"outputs":[{"name":"stdout","output_type":"stream","text":["The following columns are missing: ['Hardhat', 'Mask', 'NO-Hardhat', 'NO-Mask', 'NO-Safety Vest', 'Person', 'Safety Cone', 'Safety Vest', 'machinery', 'vehicle']\n"]}],"source":["cols = [\"Hardhat\", \"Mask\", \"NO-Hardhat\", \"NO-Mask\", \"NO-Safety Vest\", \"Person\", \"Safety Cone\", \"Safety Vest\", \"machinery\", \"vehicle\"]\n","\n","missing_cols = [col for col in cols if col not in train_csv.columns]\n","\n","if missing_cols:\n","    print(\"The following columns are missing:\", missing_cols)\n","else:\n","    print(\"All columns are present.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tIgrwlxkL3x0"},"outputs":[],"source":["\n","selected_cols = train_csv.iloc[:, 1:]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":256,"status":"ok","timestamp":1708218059291,"user":{"displayName":"Hamzaoui Mohamed Amine","userId":"09072350114090446150"},"user_tz":-60},"id":"cvDd8oeOB2kX","outputId":"767812ce-f96d-4b58-e291-15caea903362"},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 2603 validated image filenames.\n"]}],"source":["datagen=ImageDataGenerator(rescale=1./255.)\n","train_generator = datagen.flow_from_dataframe(\n","    dataframe=train_csv,\n","    directory=\"/content/extracted_folder/data/train\",  # Replace with your actual image directory\n","    x_col=\"filename\",  # Column name for image filenames\n","\n","    y_col=selected_cols.columns.tolist(),\n","    batch_size=32,\n","    seed=42,\n","    shuffle=True,\n","    class_mode=\"multi_output\",  # Use \"binary\" for binary labels\n","    target_size=(640, 640)  # Adjust based on your image dimensions\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14416,"status":"ok","timestamp":1708216439980,"user":{"displayName":"Hamzaoui Mohamed Amine","userId":"09072350114090446150"},"user_tz":-60},"id":"OogHcGe-Vvap","outputId":"a0472b13-f6e3-48d6-d645-38c4ddffed52"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.9.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n"]}],"source":["!pip install transformers\n","\n","# Import necessary libraries\n","from transformers import AutoModel, AutoTokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8Rl2ZiGjX9ds"},"outputs":[],"source":["model_path ='/content/pytorch_model (2).bin'"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7787,"status":"ok","timestamp":1708238227748,"user":{"displayName":"Hamzaoui Mohamed Amine","userId":"09072350114090446150"},"user_tz":-60},"id":"g9bt895tgZLl","outputId":"7b7d3de5-d11a-4317-858d-181db95fbaf2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (0.9.12)\n","Requirement already satisfied: torch>=1.7 in /usr/local/lib/python3.10/dist-packages (from timm) (2.1.0+cu121)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.16.0+cu121)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.1)\n","Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from timm) (0.20.3)\n","Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm) (0.4.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.13.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (4.9.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.1.3)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (2.1.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (2.31.0)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (4.66.2)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (23.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (1.25.2)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (9.4.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.7->timm) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (2024.2.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.7->timm) (1.3.0)\n"]}],"source":["pip install timm"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":257,"referenced_widgets":["82e3fe7566b1408ebd61fa0d67984da5","56740066af6d44b0a9fa76fad4dbf706","05a5b2854bb54d688177f3c12899f1b6","316314a22a984ed0843ea007ad3c7634","3b7ee2d520f44599a470f117d2654e73","736787b0405543dd93bf380a5ca29d7f","bcd73f0ac56944fcb565af7a05fa1cc2","a70fd46882194dac8b65cfa7606379a2","03a7cfdfe76f4dc0bc9fe7b2c854b584","3d799391e0f54339bbbc93b2a6e7b9b9","b11af11840614b7aa54d4fcf13a94d10","9d1573e2ef7b46ea8179962e356935eb","7e845331361e4df5a64616d88598dc5b","59cf39806b2c45a3bf03081d038312d3","ec3dc6cd832b4fc5b548a4f96f5b9599","fb41829cdcdc4a3083800c0f3586ca9b","94ae60f92eaa4549929a4f7e4e03e289","6e32e2013a044331a24b61b1abae4a26","cfaedb40eafb433ca31ebaf10157609c","10fe699df0784d928812f17a507da136","e237bf285b874dc19efd25581c53e02f","e251954bcd1548f289206e130db36980"]},"executionInfo":{"elapsed":51190,"status":"ok","timestamp":1708216569489,"user":{"displayName":"Hamzaoui Mohamed Amine","userId":"09072350114090446150"},"user_tz":-60},"id":"giMsforFYrsx","outputId":"5ec24a33-831a-4b52-d9dd-64f58d1e5639"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"82e3fe7566b1408ebd61fa0d67984da5","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/102M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at Eshwar14/ppe_yolos_model were not used when initializing DetrForObjectDetection: ['model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer1.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked']\n","- This IS expected if you are initializing DetrForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DetrForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9d1573e2ef7b46ea8179962e356935eb","version_major":2,"version_minor":0},"text/plain":["preprocessor_config.json:   0%|          | 0.00/401 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["from transformers import pipeline\n","\n","pipe = pipeline(\"object-detection\", model=\"Eshwar14/ppe_yolos_model\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8123,"status":"ok","timestamp":1708218031397,"user":{"displayName":"Hamzaoui Mohamed Amine","userId":"09072350114090446150"},"user_tz":-60},"id":"4EXp3BqSg9EV","outputId":"ffbd6c86-ee9e-4d69-dd01-1f68855ce409"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n","Some weights of the model checkpoint at Eshwar14/ppe_yolos_model were not used when initializing DetrForObjectDetection: ['model.backbone.conv_encoder.model.layer1.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked']\n","- This IS expected if you are initializing DetrForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DetrForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}],"source":["# Load model directly\n","from transformers import AutoImageProcessor, AutoModelForObjectDetection\n","\n","processor = AutoImageProcessor.from_pretrained(\"Eshwar14/ppe_yolos_model\")\n","model = AutoModelForObjectDetection.from_pretrained(\"Eshwar14/ppe_yolos_model\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EKiUZaQdnN7p"},"outputs":[],"source":["import torch\n","from transformers import AutoModelForObjectDetection, Trainer, TrainingArguments\n","from torch.utils.data import DataLoader\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":43844,"status":"ok","timestamp":1708227587034,"user":{"displayName":"Hamzaoui Mohamed Amine","userId":"09072350114090446150"},"user_tz":-60},"id":"M05GbpIFtBy0","outputId":"c9cb3b6d-3026-440b-c2af-76691b6cf908"},"outputs":[{"name":"stdout","output_type":"stream","text":["Batch 0: Data shape: torch.Size([32, 3, 640, 640]), Target shape: torch.Size([32, 10])\n","Batch 1: Data shape: torch.Size([32, 3, 640, 640]), Target shape: torch.Size([32, 10])\n","Batch 2: Data shape: torch.Size([32, 3, 640, 640]), Target shape: torch.Size([32, 10])\n","Batch 3: Data shape: torch.Size([32, 3, 640, 640]), Target shape: torch.Size([32, 10])\n","Batch 4: Data shape: torch.Size([32, 3, 640, 640]), Target shape: torch.Size([32, 10])\n","Batch 5: Data shape: torch.Size([32, 3, 640, 640]), Target shape: torch.Size([32, 10])\n","Batch 6: Data shape: torch.Size([32, 3, 640, 640]), Target shape: torch.Size([32, 10])\n","Batch 7: Data shape: torch.Size([32, 3, 640, 640]), Target shape: torch.Size([32, 10])\n","Batch 8: Data shape: torch.Size([32, 3, 640, 640]), Target shape: torch.Size([32, 10])\n","Batch 9: Data shape: torch.Size([32, 3, 640, 640]), Target shape: torch.Size([32, 10])\n","Batch 10: Data shape: torch.Size([32, 3, 640, 640]), Target shape: torch.Size([32, 10])\n","Batch 11: Data shape: torch.Size([32, 3, 640, 640]), Target shape: torch.Size([32, 10])\n","Batch 12: Data shape: torch.Size([32, 3, 640, 640]), Target shape: torch.Size([32, 10])\n","Batch 13: Data shape: torch.Size([32, 3, 640, 640]), Target shape: torch.Size([32, 10])\n","Batch 14: Data shape: torch.Size([32, 3, 640, 640]), Target shape: torch.Size([32, 10])\n","Batch 15: Data shape: torch.Size([32, 3, 640, 640]), Target shape: torch.Size([32, 10])\n","Batch 16: Data shape: torch.Size([32, 3, 640, 640]), Target shape: torch.Size([32, 10])\n","Batch 17: Data shape: torch.Size([32, 3, 640, 640]), Target shape: torch.Size([32, 10])\n","Batch 18: Data shape: torch.Size([32, 3, 640, 640]), Target shape: torch.Size([32, 10])\n","Batch 19: Data shape: torch.Size([32, 3, 640, 640]), Target shape: torch.Size([32, 10])\n","Batch 20: Data shape: torch.Size([32, 3, 640, 640]), Target shape: torch.Size([32, 10])\n","Batch 21: Data shape: torch.Size([32, 3, 640, 640]), Target shape: torch.Size([32, 10])\n","Batch 22: Data shape: torch.Size([32, 3, 640, 640]), Target shape: torch.Size([32, 10])\n","Batch 23: Data shape: torch.Size([32, 3, 640, 640]), Target shape: torch.Size([32, 10])\n","Batch 24: Data shape: torch.Size([32, 3, 640, 640]), Target shape: torch.Size([32, 10])\n","Batch 25: Data shape: torch.Size([32, 3, 640, 640]), Target shape: torch.Size([32, 10])\n","Batch 26: Data shape: torch.Size([32, 3, 640, 640]), Target shape: torch.Size([32, 10])\n","Batch 27: Data shape: torch.Size([32, 3, 640, 640]), Target shape: torch.Size([32, 10])\n","Batch 28: Data shape: torch.Size([32, 3, 640, 640]), Target shape: torch.Size([32, 10])\n","Batch 29: Data shape: torch.Size([32, 3, 640, 640]), Target shape: torch.Size([32, 10])\n","Batch 30: Data shape: torch.Size([32, 3, 640, 640]), Target shape: torch.Size([32, 10])\n","Batch 31: Data shape: torch.Size([32, 3, 640, 640]), Target shape: torch.Size([32, 10])\n","Batch 32: Data shape: torch.Size([32, 3, 640, 640]), Target shape: torch.Size([32, 10])\n","Batch 33: Data shape: torch.Size([32, 3, 640, 640]), Target shape: torch.Size([32, 10])\n","Batch 34: Data shape: torch.Size([32, 3, 640, 640]), Target shape: torch.Size([32, 10])\n","Batch 35: Data shape: torch.Size([32, 3, 640, 640]), Target shape: torch.Size([32, 10])\n","Batch 36: Data shape: torch.Size([32, 3, 640, 640]), Target shape: torch.Size([32, 10])\n","Batch 37: Data shape: torch.Size([32, 3, 640, 640]), Target shape: torch.Size([32, 10])\n","Batch 38: Data shape: torch.Size([32, 3, 640, 640]), Target shape: torch.Size([32, 10])\n","Batch 39: Data shape: torch.Size([32, 3, 640, 640]), Target shape: torch.Size([32, 10])\n","Batch 40: Data shape: torch.Size([32, 3, 640, 640]), Target shape: torch.Size([32, 10])\n","Batch 41: Data shape: torch.Size([32, 3, 640, 640]), Target shape: torch.Size([32, 10])\n","Batch 42: Data shape: torch.Size([32, 3, 640, 640]), Target shape: torch.Size([32, 10])\n","Batch 43: Data shape: torch.Size([32, 3, 640, 640]), Target shape: torch.Size([32, 10])\n","Batch 44: Data shape: torch.Size([32, 3, 640, 640]), Target shape: torch.Size([32, 10])\n","Batch 45: Data shape: torch.Size([32, 3, 640, 640]), Target shape: torch.Size([32, 10])\n","Batch 46: Data shape: torch.Size([32, 3, 640, 640]), Target shape: torch.Size([32, 10])\n","Batch 47: Data shape: torch.Size([32, 3, 640, 640]), Target shape: torch.Size([32, 10])\n","Batch 48: Data shape: torch.Size([32, 3, 640, 640]), Target shape: torch.Size([32, 10])\n","Batch 49: Data shape: torch.Size([32, 3, 640, 640]), Target shape: torch.Size([32, 10])\n","Batch 50: Data shape: torch.Size([32, 3, 640, 640]), Target shape: torch.Size([32, 10])\n","Batch 51: Data shape: torch.Size([32, 3, 640, 640]), Target shape: torch.Size([32, 10])\n","Batch 52: Data shape: torch.Size([32, 3, 640, 640]), Target shape: torch.Size([32, 10])\n","Batch 53: Data shape: torch.Size([32, 3, 640, 640]), Target shape: torch.Size([32, 10])\n","Batch 54: Data shape: torch.Size([32, 3, 640, 640]), Target shape: torch.Size([32, 10])\n","Batch 55: Data shape: torch.Size([32, 3, 640, 640]), Target shape: torch.Size([32, 10])\n","Batch 56: Data shape: torch.Size([32, 3, 640, 640]), Target shape: torch.Size([32, 10])\n","Batch 57: Data shape: torch.Size([32, 3, 640, 640]), Target shape: torch.Size([32, 10])\n","Batch 58: Data shape: torch.Size([32, 3, 640, 640]), Target shape: torch.Size([32, 10])\n","Batch 59: Data shape: torch.Size([32, 3, 640, 640]), Target shape: torch.Size([32, 10])\n","Batch 60: Data shape: torch.Size([32, 3, 640, 640]), Target shape: torch.Size([32, 10])\n","Batch 61: Data shape: torch.Size([32, 3, 640, 640]), Target shape: torch.Size([32, 10])\n","Batch 62: Data shape: torch.Size([32, 3, 640, 640]), Target shape: torch.Size([32, 10])\n","Batch 63: Data shape: torch.Size([32, 3, 640, 640]), Target shape: torch.Size([32, 10])\n","Batch 64: Data shape: torch.Size([32, 3, 640, 640]), Target shape: torch.Size([32, 10])\n","Batch 65: Data shape: torch.Size([32, 3, 640, 640]), Target shape: torch.Size([32, 10])\n","Batch 66: Data shape: torch.Size([32, 3, 640, 640]), Target shape: torch.Size([32, 10])\n","Batch 67: Data shape: torch.Size([32, 3, 640, 640]), Target shape: torch.Size([32, 10])\n","Batch 68: Data shape: torch.Size([32, 3, 640, 640]), Target shape: torch.Size([32, 10])\n","Batch 69: Data shape: torch.Size([32, 3, 640, 640]), Target shape: torch.Size([32, 10])\n","Batch 70: Data shape: torch.Size([32, 3, 640, 640]), Target shape: torch.Size([32, 10])\n","Batch 71: Data shape: torch.Size([32, 3, 640, 640]), Target shape: torch.Size([32, 10])\n","Batch 72: Data shape: torch.Size([32, 3, 640, 640]), Target shape: torch.Size([32, 10])\n","Batch 73: Data shape: torch.Size([32, 3, 640, 640]), Target shape: torch.Size([32, 10])\n","Batch 74: Data shape: torch.Size([32, 3, 640, 640]), Target shape: torch.Size([32, 10])\n","Batch 75: Data shape: torch.Size([32, 3, 640, 640]), Target shape: torch.Size([32, 10])\n","Batch 76: Data shape: torch.Size([32, 3, 640, 640]), Target shape: torch.Size([32, 10])\n","Batch 77: Data shape: torch.Size([32, 3, 640, 640]), Target shape: torch.Size([32, 10])\n","Batch 78: Data shape: torch.Size([32, 3, 640, 640]), Target shape: torch.Size([32, 10])\n","Batch 79: Data shape: torch.Size([32, 3, 640, 640]), Target shape: torch.Size([32, 10])\n","Batch 80: Data shape: torch.Size([32, 3, 640, 640]), Target shape: torch.Size([32, 10])\n","Batch 81: Data shape: torch.Size([11, 3, 640, 640]), Target shape: torch.Size([11, 10])\n"]}],"source":["import torch\n","from torch.utils.data import Dataset, DataLoader\n","from PIL import Image\n","from torchvision import transforms\n","import pandas as pd\n","from transformers import DataCollator\n","\n","class CustomDataset(Dataset):\n","    def __init__(self, dataframe, directory, transform=None):\n","        self.dataframe = dataframe\n","        self.directory = directory\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.dataframe)\n","\n","    def __getitem__(self, idx):\n","        img_name = self.dataframe.iloc[idx, 0]  # Assuming the filename is in the first column\n","        img_path = os.path.join(self.directory, img_name)\n","        image = Image.open(img_path)\n","        labels = self.dataframe.iloc[idx, 1:].values.astype(float)  # Assuming labels start from the second column\n","        labels = torch.tensor(labels, dtype=torch.float32)\n","\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        return image, labels\n","\n","\n","# Define transformations to be applied to your data\n","transform = transforms.Compose([\n","    transforms.Resize((640, 640)),\n","    transforms.ToTensor(),\n","])\n","\n","# Create an instance of CustomDataset\n","train_dataset = CustomDataset(\n","    dataframe=train_csv,\n","    directory=\"/content/extracted_folder/data/train\",\n","    transform=transform,\n",")\n","\n","\n","\n","# Create a DataLoader with the custom collate function\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","\n","# Example usage of train_loader\n","for batch_idx, (data, target) in enumerate(train_loader):\n","    print(f\"Batch {batch_idx}: Data shape: {data.shape}, Target shape: {target.shape}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uleu0vMo14fJ"},"outputs":[],"source":["\"\"\"#MODIF 1\n","class CustomDataset(Dataset):\n","    def __init__(self, dataframe, directory, transform=None):\n","        self.dataframe = dataframe\n","        self.directory = directory\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.dataframe)\n","\n","    def __getitem__(self, idx):\n","        img_name = self.dataframe.iloc[idx, 0]  # Assuming the filename is in the first column\n","        img_path = os.path.join(self.directory, img_name)\n","        image = Image.open(img_path)\n","        labels = self.dataframe.iloc[idx, 1:].values.astype(float)  # Assuming labels start from the second column\n","        labels = torch.tensor(labels, dtype=torch.float32)\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        return {'inputs': image, 'labels': labels}  # Change 'image' to 'inputs'\n","\n","# Define transformations to be applied to your data\n","transform = transforms.Compose([\n","    transforms.Resize((640, 640)),\n","    transforms.ToTensor(),\n","])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JaOa_4m5m3fe"},"outputs":[],"source":["import torch\n","from transformers import AutoModelForObjectDetection, Trainer, TrainingArguments\n","from torch.utils.data import DataLoader\n","learning_rate = 1e-5\n","train_batch_size = 8\n","eval_batch_size = 8\n","seed = 42\n","num_epochs = 1\n","mixed_precision_training = True\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Define training arguments\n","training_args = TrainingArguments(\n","    output_dir=\"/content/model\",  # Specify the directory to save the trained model and logs\n","    overwrite_output_dir=True,\n","    learning_rate=learning_rate,\n","    per_device_train_batch_size=train_batch_size,\n","    per_device_eval_batch_size=eval_batch_size,\n","    num_train_epochs=num_epochs,\n","    seed=seed,\n","    evaluation_strategy=\"epoch\",\n","    logging_dir=\"./logs\",  # Specify the directory to save training logs\n","    report_to=\"none\",  # Disable report generation\n","    logging_steps=100,  # Log every 100 steps\n","    save_strategy=\"epoch\",  # Save model checkpoints every epoch\n","    save_steps=500,  # Save checkpoints every 500 steps\n","    save_total_limit=3,\n","    fp16=False,  # Disable mixed precision training\n","    fp16_full_eval=False,  # Disable full precision evaluation# Limit the total number of saved checkpoints  # Enable mixed precision training if supported\n","    logging_first_step=True,\n","    lr_scheduler_type=\"linear\",\n","    warmup_steps=0,  # No warmup steps\n","    weight_decay=0.01,  # Weight decay coefficient\n","\n",")\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":321},"executionInfo":{"elapsed":1392,"status":"error","timestamp":1708227409161,"user":{"displayName":"Hamzaoui Mohamed Amine","userId":"09072350114090446150"},"user_tz":-60},"id":"e7L-sfhazcOg","outputId":"78ec5063-ad6d-4374-f1e5-b185f8ebbcf9"},"outputs":[{"ename":"TypeError","evalue":"vars() argument must have __dict__ attribute","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-84-9a0f2fb55c14>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1553\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1554\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1555\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   1556\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1557\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1836\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1837\u001b[0m             \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1838\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1839\u001b[0m                 \u001b[0mtotal_batched_samples\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1840\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mrng_to_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/data_loader.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    450\u001b[0m         \u001b[0;31m# We iterate one batch ahead to check when we are at the end\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m             \u001b[0mcurrent_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m             \u001b[0;32myield\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 674\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    675\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer_utils.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    753\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_remove_columns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 755\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_collator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/data/data_collator.py\u001b[0m in \u001b[0;36mdefault_data_collator\u001b[0;34m(features, return_tensors)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_tensors\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch_default_data_collator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mreturn_tensors\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"tf\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtf_default_data_collator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/data/data_collator.py\u001b[0m in \u001b[0;36mtorch_default_data_collator\u001b[0;34m(features)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m     \u001b[0mfirst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/data/data_collator.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m     \u001b[0mfirst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: vars() argument must have __dict__ attribute"]}],"source":["trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n",")\n","\n","# Train the model\n","trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18679,"status":"ok","timestamp":1708217822633,"user":{"displayName":"Hamzaoui Mohamed Amine","userId":"09072350114090446150"},"user_tz":-60},"id":"CAuNn0Outx0Z","outputId":"e874f34e-a42b-47fc-cea3-9a630bfb7ff6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting accelerate\n","  Downloading accelerate-0.27.2-py3-none-any.whl (279 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m280.0/280.0 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n","Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu121)\n","Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.20.3)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.9.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.3)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n","Installing collected packages: accelerate\n","Successfully installed accelerate-0.27.2\n"]}],"source":["pip install accelerate -U"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pFPrtwUuwqkJ"},"outputs":[],"source":["\"\"\"# MODIF 0\n","import torch\n","from transformers import AutoModelForObjectDetection, Trainer, TrainingArguments\n","from torch.utils.data import DataLoader\n","from PIL import Image\n","from torchvision import transforms\n","import pandas as pd\n","import os\n","\n","\n","\n","# Define your training dataset class\n","class CustomDataset(Dataset):\n","    def __init__(self, dataframe, directory, transform=None):\n","        self.dataframe = dataframe\n","        self.directory = directory\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.dataframe)\n","\n","    def __getitem__(self, idx):\n","        img_name = self.dataframe.iloc[idx, 0]  # Assuming the filename is in the first column\n","        img_path = os.path.join(self.directory, img_name)\n","        image = Image.open(img_path)\n","        labels = self.dataframe.iloc[idx, 1:].values.astype(float)  # Assuming labels start from the second column\n","        labels = torch.tensor(labels, dtype=torch.float32)\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        return image, labels\n","\n","# Define transformations to be applied to your data\n","transform = transforms.Compose([\n","    transforms.Resize((640, 640)),\n","    transforms.ToTensor(),\n","])\n","\n","# Create an instance of CustomDataset\n","train_dataset = CustomDataset(\n","    dataframe=train_csv,\n","    directory=\"/content/extracted_folder/data/train\",\n","    transform=transform,\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":321},"executionInfo":{"elapsed":960,"status":"error","timestamp":1708218720109,"user":{"displayName":"Hamzaoui Mohamed Amine","userId":"09072350114090446150"},"user_tz":-60},"id":"V4D6_boew1-G","outputId":"12d9bf83-5e7e-429c-d9b0-1ef2748a48ac"},"outputs":[{"ename":"TypeError","evalue":"vars() argument must have __dict__ attribute","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-21-5427295135c0>\u001b[0m in \u001b[0;36m<cell line: 39>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1553\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1554\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1555\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   1556\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1557\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1836\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1837\u001b[0m             \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1838\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1839\u001b[0m                 \u001b[0mtotal_batched_samples\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1840\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mrng_to_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/data_loader.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    450\u001b[0m         \u001b[0;31m# We iterate one batch ahead to check when we are at the end\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m             \u001b[0mcurrent_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m             \u001b[0;32myield\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 674\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    675\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer_utils.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    753\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_remove_columns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 755\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_collator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/data/data_collator.py\u001b[0m in \u001b[0;36mdefault_data_collator\u001b[0;34m(features, return_tensors)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_tensors\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch_default_data_collator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mreturn_tensors\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"tf\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtf_default_data_collator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/data/data_collator.py\u001b[0m in \u001b[0;36mtorch_default_data_collator\u001b[0;34m(features)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m     \u001b[0mfirst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/data/data_collator.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m     \u001b[0mfirst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: vars() argument must have __dict__ attribute"]}],"source":["\"\"\"# Define training arguments\n","learning_rate = 1e-5\n","train_batch_size = 8\n","eval_batch_size = 8\n","seed = 42\n","num_epochs = 1\n","\n","training_args = TrainingArguments(\n","    output_dir=\"/content/model\",\n","    overwrite_output_dir=True,\n","    learning_rate=learning_rate,\n","    per_device_train_batch_size=train_batch_size,\n","    per_device_eval_batch_size=eval_batch_size,\n","    num_train_epochs=num_epochs,\n","    seed=seed,\n","    evaluation_strategy=\"epoch\",\n","    logging_dir=\"./logs\",\n","    report_to=\"none\",\n","    logging_steps=100,\n","    save_strategy=\"epoch\",\n","    save_steps=500,\n","    save_total_limit=3,\n","    fp16=False,\n","    fp16_full_eval=False,\n","    logging_first_step=True,\n","    lr_scheduler_type=\"linear\",\n","    warmup_steps=0,\n","    weight_decay=0.01,\n",")\n","\n","# Instantiate the Trainer class\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,  # Pass train_dataset instead of train_loader\n",")\n","\n","# Train the model\n","trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WeKZQWThp5WO"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"03a7cfdfe76f4dc0bc9fe7b2c854b584":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"05a5b2854bb54d688177f3c12899f1b6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a70fd46882194dac8b65cfa7606379a2","max":102469840,"min":0,"orientation":"horizontal","style":"IPY_MODEL_03a7cfdfe76f4dc0bc9fe7b2c854b584","value":102469840}},"10fe699df0784d928812f17a507da136":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"316314a22a984ed0843ea007ad3c7634":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3d799391e0f54339bbbc93b2a6e7b9b9","placeholder":"​","style":"IPY_MODEL_b11af11840614b7aa54d4fcf13a94d10","value":" 102M/102M [00:01&lt;00:00, 93.9MB/s]"}},"3b7ee2d520f44599a470f117d2654e73":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3d799391e0f54339bbbc93b2a6e7b9b9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"56740066af6d44b0a9fa76fad4dbf706":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_736787b0405543dd93bf380a5ca29d7f","placeholder":"​","style":"IPY_MODEL_bcd73f0ac56944fcb565af7a05fa1cc2","value":"model.safetensors: 100%"}},"59cf39806b2c45a3bf03081d038312d3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_cfaedb40eafb433ca31ebaf10157609c","max":401,"min":0,"orientation":"horizontal","style":"IPY_MODEL_10fe699df0784d928812f17a507da136","value":401}},"6e32e2013a044331a24b61b1abae4a26":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"736787b0405543dd93bf380a5ca29d7f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7e845331361e4df5a64616d88598dc5b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_94ae60f92eaa4549929a4f7e4e03e289","placeholder":"​","style":"IPY_MODEL_6e32e2013a044331a24b61b1abae4a26","value":"preprocessor_config.json: 100%"}},"82e3fe7566b1408ebd61fa0d67984da5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_56740066af6d44b0a9fa76fad4dbf706","IPY_MODEL_05a5b2854bb54d688177f3c12899f1b6","IPY_MODEL_316314a22a984ed0843ea007ad3c7634"],"layout":"IPY_MODEL_3b7ee2d520f44599a470f117d2654e73"}},"94ae60f92eaa4549929a4f7e4e03e289":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9d1573e2ef7b46ea8179962e356935eb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7e845331361e4df5a64616d88598dc5b","IPY_MODEL_59cf39806b2c45a3bf03081d038312d3","IPY_MODEL_ec3dc6cd832b4fc5b548a4f96f5b9599"],"layout":"IPY_MODEL_fb41829cdcdc4a3083800c0f3586ca9b"}},"a70fd46882194dac8b65cfa7606379a2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b11af11840614b7aa54d4fcf13a94d10":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bcd73f0ac56944fcb565af7a05fa1cc2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cfaedb40eafb433ca31ebaf10157609c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e237bf285b874dc19efd25581c53e02f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e251954bcd1548f289206e130db36980":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ec3dc6cd832b4fc5b548a4f96f5b9599":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e237bf285b874dc19efd25581c53e02f","placeholder":"​","style":"IPY_MODEL_e251954bcd1548f289206e130db36980","value":" 401/401 [00:00&lt;00:00, 5.17kB/s]"}},"fb41829cdcdc4a3083800c0f3586ca9b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":0}